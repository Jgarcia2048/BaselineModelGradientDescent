{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Group3_RecSystems.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEMzxVpxglfe",
        "outputId": "33d18fcc-d038-4d63-d249-3cea9ae80029",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDCLIfht-3tv"
      },
      "source": [
        "#ADD ANY IMPORTS HERE\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math \n",
        "import random\n",
        "import time\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "import plotly.graph_objects as go"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PM4BXEFW8VgA"
      },
      "source": [
        "def PreprocessData():\n",
        "  # READ THE DATA IN FROM CSV FOR AND TRANSFORMING IT TO MATRIX COLUMNS ARE BUSINESS IDS AND ROWS ARE USERS\n",
        "  df = pd.read_csv('/content/drive/My Drive/CS419 FINAL PROJECT GROUP 3/Yelp/ratings.csv', sep=',')\n",
        "  \n",
        "  # drop the date since we aren't using it \n",
        "  df.drop([\"Date\"], axis=1, inplace=True)\n",
        "  df.sort_values(by=[\"User ID\"], inplace=True)\n",
        "  df = df.reset_index(drop=True)\n",
        "\n",
        "  counter = Counter(df[\"User ID\"].values)\n",
        "  test_df = pd.DataFrame()\n",
        "  # SEPARATE TRAINING DATA AND TEST DATA \n",
        "  for user in counter:\n",
        "      if counter[user] >= 2:\n",
        "          get_this_many = math.floor(counter[user] * .8)\n",
        "          sample_df = df[df[\"User ID\"] == user].sample(n=get_this_many)\n",
        "          df = df.drop(sample_df.index)\n",
        "          test_df = pd.concat([test_df, sample_df])\n",
        "          if len(test_df) >= 6748:\n",
        "              break\n",
        "\n",
        "  train = df.pivot_table(index=\"User ID\", values=\"Rating\", columns=\"Business ID\")\n",
        "  test = test_df.pivot_table(index=\"User ID\", values=\"Rating\", columns=\"Business ID\")\n",
        "  \n",
        "  # MovieLens Data \n",
        "  mldf = pd.read_csv('/content/drive/My Drive/CS419 FINAL PROJECT GROUP 3/MovieLens/u1.base', sep='\\t', names=[\"User ID\", \"Item ID\", \"Rating\", \"Timestamp\"])\n",
        "  mldf.drop([\"Timestamp\"], axis=1, inplace=True)\n",
        "  ML_Test = pd.read_csv('/content/drive/My Drive/CS419 FINAL PROJECT GROUP 3/MovieLens/u1.test', sep='\\t', names=[\"User ID\", \"Item ID\", \"Rating\", \"Timestamp\"])\n",
        "  ML_Test.drop([\"Timestamp\"], axis=1, inplace=True)\n",
        "  trainML = mldf.pivot_table(index=\"User ID\", values=\"Rating\", columns=\"Item ID\")\n",
        "  testML = ML_Test.pivot_table(index=\"User ID\", values=\"Rating\", columns=\"Item ID\")\n",
        "\n",
        "  return train, test, trainML, testML"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34K8BDpwEa8s"
      },
      "source": [
        "\"\"\" Jesus Garcia 7/21/20\"\"\"\n",
        "def BaselineStatisticalMethod(Train, Test):\n",
        "  itm_id_train = Train.columns.values.tolist() # A list of item ID numbers to verify that the rating we observe belongs to a specified item or business ID\n",
        "  itm_id_test = Test.columns.values.tolist()\n",
        "  usr_id_train = Train.index.values.tolist()\n",
        "  usr_id_test = Test.index.values.tolist()\n",
        "\n",
        "  Train = Train.to_numpy() # Convert Train-setDataframe to Numpy Array\n",
        "  Test = Test.to_numpy()# Convert Test-set Dataframe to Numpy Array\n",
        "\n",
        "  i, j = Train.shape # Training Dataset Dimension RowxCol\n",
        "  ti, tj = Test.shape # Test Dataset Dimension RowxCol\n",
        "  u = np.nanmean(Train) # Training Dataset observed rating mean\n",
        "\n",
        "  usr_means = np.nanmean(Train, axis=1,dtype=float) # A list of the avg of ratings for each user (mean(bu's))\n",
        "  itm_means = np.nanmean(Train, axis=0,dtype=float) # A list of the avg of each item's ratings (mean(bi's))\n",
        "  usr_dev = [rt - u for rt in usr_means] # A list of deviations from a user's average to the training set observed ratings average\n",
        "  itm_dev = [rt - u for rt in itm_means] # A list of deviations from a items's average to the training set observed ratings average\n",
        "\n",
        "  matrix = BuildPredictionMatrix(Test, usr_dev, itm_dev, itm_id_train, itm_id_test, usr_id_train, usr_id_test, u)\n",
        "  MAE = (CalculateMAE(Test, matrix, u))\n",
        "\n",
        "  return MAE\n",
        "\n",
        "\n",
        "def BuildPredictionMatrix(TestData, usr_dev, itm_dev, itm_id_train, itm_id_test, usr_id_train, usr_id_test, u):\n",
        "  \"\"\" This method aims to take the training data\n",
        "      (bu and bi deviations and global mean)\n",
        "      and create a matrix for predictions that \n",
        "      will be later evaluated for its similarity \n",
        "      to the test set.\n",
        "\n",
        "      NOTE:The only reason the test data is \n",
        "      passed in is to observe the position \n",
        "      of where to make the prediction!\n",
        "      \n",
        "      \"\"\"\n",
        "  ti, tj = TestData.shape\n",
        "  prediction_matrix = np.empty((ti, tj))\n",
        "  prediction_matrix[:] = np.NaN\n",
        "  \n",
        "  for row in range(0,ti):\n",
        "    for col in range(0, tj):   \n",
        "      if(math.isnan(TestData[row][col]) == False): # TestSet is utilized to observe position of where a prediction is needed\n",
        "          for x in range(0, len(itm_id_train)): # This loop checks the training set's item ids to find the correct deviation for the item on the test set\n",
        "            if(itm_id_test[col] == itm_id_train[x]):\n",
        "              for y in range(0, len(usr_id_train)): # This loop checks the training set's user ids to find the correct deviation for the user on the test set\n",
        "                if(usr_id_test[row] == usr_id_train[y]):\n",
        "                  bi = itm_dev[x]\n",
        "                  bu = usr_dev[y]\n",
        "                  prediction_matrix[row][col] = u + bu + bi # Prediction is made based on the sum of the training set mean, bu & bi deviations to each respective user and item\n",
        "\n",
        "                  # if(prediction_matrix[row][col] > 5):\n",
        "                  #   prediction_matrix[row][col] = 5\n",
        "                  # elif(prediction_matrix[row][col] < 0):\n",
        "                  #   prediction_matrix[row][col] = 0\n",
        "  return prediction_matrix\n",
        "\n",
        "\n",
        "def CalculateMAE(Actual, Predicted, u):\n",
        "    ti, tj = Actual.shape\n",
        "    n = 0\n",
        "    sum = 0\n",
        "\n",
        "    for row in range(0,ti):\n",
        "      for col in range(0, tj):   \n",
        "        if(math.isnan(Actual[row][col]) == False and math.isnan(Predicted[row][col]) == False):\n",
        "          sum = sum + abs((Actual[row][col] - Predicted[row][col]))\n",
        "          n += 1\n",
        "\n",
        "    return round((sum/n),3)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lw4cNsg63GMC"
      },
      "source": [
        "\"\"\" Jesus Garcia 7/25/20 \"\"\"\n",
        "def BaselineGradientDescent(Train, Test, DatasetName, iterations):\n",
        "\n",
        "  itm_id_train = Train.columns.values.tolist() # A list of item ID numbers to verify that the rating we observe belongs to a specified item or business ID\n",
        "  itm_id_test = Test.columns.values.tolist()\n",
        "  usr_id_train = Train.index.values.tolist() # A list of user ID numbers to verify that the rating we observe belongs to a specified user ID\n",
        "  usr_id_test = Test.index.values.tolist()\n",
        "\n",
        "  Train = Train.to_numpy() # Convert Train-setDataframe to Numpy Array\n",
        "  Test = Test.to_numpy() # Convert Test-set Dataframe to Numpy Array\n",
        "  i, j = Train.shape # Dimensions of Training Data Row x Col\n",
        "\n",
        "  # info = np.empty((4, 5))\n",
        "  # info[:] = np.NaN\n",
        "  # info[0][0] = 3\n",
        "  # info[0][1] = 2\n",
        "  # info[0][2] = 5\n",
        "  # info[0][3] = 3\n",
        "  # info[1][1] = 1\n",
        "  # info[1][3] = 5\n",
        "  # info[1][4] = 2\n",
        "  # info[2][0] = 3\n",
        "  # info[2][2] = 2\n",
        "  # info[2][3] = 2\n",
        "  # info[3][0] = 1\n",
        "  # info[3][2] = 3\n",
        "  # info[3][4] = 2\n",
        "  # i, j = info.shape\n",
        "\n",
        "  u = round(np.nanmean(Train), 3)\n",
        "  bu_lst = [random.randrange(0, 5, 1) for i in range(i)]\n",
        "  bi_lst = [random.randrange(0, 5, 1) for i in range(j)]\n",
        "\n",
        "  usr_dev = [rt for rt in bu_lst] # Initial Bu\n",
        "  itm_dev = [rt for rt in bi_lst] # Initial Bi\n",
        "  iter_list = []\n",
        "  iter_list.append(0)\n",
        "  COST_LIST = []\n",
        "  learning_rate = 0.02\n",
        "\n",
        "  for epoch in range(0,iterations,1):\n",
        "    if(epoch == 0):\n",
        "      COST_LIST.append(CalculateLossFunction(Train, usr_dev, itm_dev)) # Store data\n",
        "    \n",
        "    for row in range(i):\n",
        "      for col in range(j):\n",
        "        if(math.isnan(Train[row][col]) == False):\n",
        "          eui = (Train[row][col] - u - usr_dev[row] - itm_dev[col])\n",
        "          usr_dev[row] = usr_dev[row] - learning_rate * (-2 * eui + (2 * 0.25 * usr_dev[row])) # Check without absolute value\n",
        "          itm_dev[col] = itm_dev[col] - learning_rate * (-2 * eui + (2 * 0.25 * itm_dev[col]))\n",
        "\n",
        "    PredictionMatrix = BuildPredictionMatrix(Test, usr_dev, itm_dev, itm_id_train, itm_id_test, usr_id_train, usr_id_test, u )\n",
        "    MAE = CalculateMAE(Test, PredictionMatrix, u)\n",
        "    # Plot val of loss func against differnt epoch\n",
        "\n",
        "    if(epoch == 0 or epoch == 4 or epoch == 9 or epoch == 49 or epoch == 99):\n",
        "      # print(\"The MAE for a training instance with \" + str(epoch + 1) + \" epoch(s) is: \" + str(MAE))\n",
        "      iter_list.append(epoch + 1)\n",
        "      COST_LIST.append(CalculateLossFunction(Train, usr_dev, itm_dev)) # Store data\n",
        "\n",
        "      if epoch == 99:\n",
        "        \n",
        "        GraphMAES(COST_LIST, iter_list, DatasetName)\n",
        "\n",
        "  return MAE\n",
        "\n",
        "def GraphMAES(COSTS, EPOCHS, Dname):\n",
        "  COSTS = np.array(COSTS, dtype=float)\n",
        "  EPOCHS = np.array(EPOCHS, dtype=float)\n",
        "  # plt.plot(EPOCHS, MAES, 'o')\n",
        " \n",
        "\n",
        "  fig = go.Figure(\n",
        "      data=go.Scatter(\n",
        "      x= EPOCHS,\n",
        "      y= COSTS,\n",
        "      marker=dict(color=\"crimson\", size=12),\n",
        "      name=Dname,\n",
        "  ))\n",
        "\n",
        "  fig.update_layout(title=\"LOSS FUNCTION COST OVER EPOCHS FOR \" + Dname + \" DATASET\",\n",
        "                    xaxis_title=\"EPOCH\",\n",
        "                    yaxis_title=\"COST\")\n",
        "\n",
        "  fig.show()\n",
        "\n",
        "\n",
        "def CalculateLossFunction(dataset, bu_dev_list, bi_dev_list):\n",
        "  i, j = dataset.shape\n",
        "  bu_dev_list = [pow(bu_dev, 2) for bu_dev in bu_dev_list] \n",
        "  bi_dev_list = [pow(bi_dev, 2) for bi_dev in bi_dev_list] \n",
        "  u = round(np.nanmean(dataset), 3)\n",
        "  hyper_parameter = 0.25 * (sum(bu_dev_list) + sum(bi_dev_list))\n",
        "  add = 0\n",
        "\n",
        "  for row in range(i):\n",
        "    for col in range(j):\n",
        "      if(math.isnan(dataset[row][col]) == False):\n",
        "        eui = math.pow((dataset[row][col] - u - bu_dev_list[row] - bi_dev_list[col]) , 2)\n",
        "        add += eui\n",
        "\n",
        "  return add + hyper_parameter "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cb1ksQYPR023",
        "outputId": "ebee67d3-cbc9-484e-8276-f5719a8d989c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def main():\n",
        "    YelpTrain, YelpTest, TrainML, TestML = PreprocessData()\n",
        "\n",
        "    df_yelp_train = pd.read_csv('/content/drive/My Drive/CS419 FINAL PROJECT GROUP 3/Yelp/formatted_yelp_train.tsv', sep='\\t', index_col=\"User ID\") # This should be all you need\n",
        "    df_yelp_test = pd.read_csv('/content/drive/My Drive/CS419 FINAL PROJECT GROUP 3/Yelp/formatted_yelp_test.tsv', sep='\\t', index_col=\"User ID\") # This should be all\n",
        "\n",
        "    df_yelp_train.head()\n",
        "    #Utilize the time package to keep track of the duration of your algorithm implementations\n",
        "    \n",
        "    # BASELINE STATISTICAL COMPUTATION\n",
        "    start = time.time()\n",
        "    MLMAE = BaselineStatisticalMethod(TrainML, TestML)\n",
        "    YPMAE = BaselineStatisticalMethod(df_yelp_train, df_yelp_test)\n",
        "    end = time.time()\n",
        "    print(\"The MAE for the baseline prediction model using statistical computation is: \" + str(MLMAE) + \" for the MovieLens Dataset\")\n",
        "    print(\"The MAE for the baseline prediction model using statistical computation is: \" + str(YPMAE) + \" for the Yelp Dataset\\n\")\n",
        "    print(\"The Statistical Prediction Baseline Model took \" + str(round(end - start, 3)) + \" seconds to run\\n \\n\")\n",
        "\n",
        "    # BASELINE GRADIENT DESCENT\n",
        "    print(\"\\n\" + \"Initializing the training of the baseline model through gradient descent\\n\")\n",
        "    # learn_rates = [0.005, 0.01, 0.015, 0.02, 0.025, 0.03, 0.035, 0.04]\n",
        "    # mae_out = np.empty((2, 8), dtype=float)\n",
        "    epochs = [1, 5, 10, 50, 100]\n",
        "    n = 0\n",
        "    epoch_maes = np.empty((2, 5)) # Row 1 is MOVIELENS data ROW 2 is Yelp data\n",
        "    start2 = time.time()\n",
        "    while n <= 4:\n",
        "      epoch_maes[0][n] = BaselineGradientDescent(TrainML, TestML, \"MOVIELENS\", epochs[n])\n",
        "      epoch_maes[1][n] = BaselineGradientDescent(df_yelp_train, df_yelp_test, \"YELP\", epochs[n])\n",
        "      print(\"One instance of the training cycle has finished with \" + str(epochs[n]) + \" epoch(s)\" )\n",
        "      n +=1\n",
        "    \n",
        "    print(\"\\n\")\n",
        "    print(\"\\n\")\n",
        "    print(\"\\n\")\n",
        "    \n",
        "    end2 = time.time()\n",
        "    print(\"\\nThe Gradient Descent Baseline Model took \" + str(round(end2 - start2, 3)) + \" seconds to run\\n\")\n",
        "    print(\"For the MovieLens Dataset, the following MAE's were recorded for the specified number of epochs for that instance:\\n\")\n",
        "    for i in range(n):\n",
        "      print(\"For an instance with \" + str(epochs[i]) + \" epoch(s) the MAE is: \" + str(epoch_maes[0][i]))\n",
        "    print()\n",
        "\n",
        "    print(\"For the Yelp Dataset, the following MAE's were recorded for the specified number of epochs for that instance:\\n\")\n",
        "    for i in range(n):\n",
        "      print(\"For an instance with \" + str(epochs[i]) + \" epoch(s) the MAE is: \" + str(epoch_maes[1][i]))\n",
        "    # np.savetxt(\"GRADIENTDESCENTMAES.txt\", np.array(mae_out), fmt=\"%s\")  # This saved the maes for the learning rates(GRADIENTDESCENTMAES.txt) in the list \"learn_rates\" and I chose the min mae for the learn rate\n",
        "\n",
        "    # ITEM BASED SIMILARITY\n",
        "    k_values = np.array([1, 5, 10, 50, 100])\n",
        "\n",
        "    print(\"Movie Lens Data\")\n",
        "    for i in range(len(k_values)):\n",
        "        print(\"Item-based similarity model for k = \" + str(k_values[i]) + \":\")\n",
        "        start_time = datetime.datetime.now()\n",
        "        item_based_similarity_model(k_values[i], MLTrain.T, MLTest.T)\n",
        "        elapsed_time = datetime.datetime.now() - start_time\n",
        "        print(\"Elapsed time: \" + str(elapsed_time) + \"\\n\")\n",
        "    \n",
        "    print(\"Yelp Data\")\n",
        "    for i in range(len(k_values)):\n",
        "        print(\"Item-based similarity model for k = \" + str(k_values[i]) + \":\")\n",
        "        start_time = datetime.datetime.now()\n",
        "        item_based_similarity_model(k_values[i], df_yelp_train.T, df_yelp_test.T)\n",
        "        elapsed_time = datetime.datetime.now() - start_time\n",
        "        print(\"Elapsed time: \" + str(elapsed_time) + \"\\n\")\n",
        "    \n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The MAE for the baseline prediction model using statistical computation is: 0.771 for the MovieLens Dataset\n",
            "The MAE for the baseline prediction model using statistical computation is: 1.071 for the Yelp Dataset\n",
            "\n",
            "The Statistical Prediction Baseline Model took 11.008 seconds to run\n",
            " \n",
            "\n",
            "\n",
            "Initializing the training of the baseline model through gradient descent\n",
            "\n",
            "One instance of the training cycle has finished with 1 epoch(s)\n",
            "One instance of the training cycle has finished with 5 epoch(s)\n",
            "One instance of the training cycle has finished with 10 epoch(s)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpQ5sGjms5kk"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDMmAA1ygOLU"
      },
      "source": [
        ""
      ]
    }
  ]
}